[TOC]
# 简单介绍word2Vec
* word2Vec是一种产生词向量的模型，通过训练可以把词映射为实数域中的向量，词义相近的词，在向量空间中的距离也应该相近，作为下游任务的补充输入。它分为两种模型：
    * skip-gram，给定中心词预测上下文,一般使用中心词向量作为表征向量
    * cbow，给定上下文预测中心词，一般使用背景词向量作为表征向量
* word2Vec一般采用近似训练来加速训练过程，有两种方法：
    * 负采样
    * 层序softmax

# 如何通过词嵌入获得短语、句子嵌入
* 求平均
* 使用tf-idf做加权平均
* 利用EMLo类似的语言模型，得到句子表征。

# word2Vec如何训练短语的向量
* 首先找出训练数据中的短语，通过score函数来判别，当score大于某一阈值，则判定为短语。
* Word2Vec中使用了窗口大小`2-4`的短语。
* 将短语替换为unique tokens之后再进行训练得到短语的vec

# word2Vec中分层softmax建立的霍夫曼树作用
* 把原来的计算复杂度V，降低成为了`log2V`
* 建立霍夫曼树之后，越高频的词越靠近根，这样找到高频词需要的时间越少，降低了平均的时间复杂度


# 二次采样
* 数据集中每个被索引词将有一定概率被丢弃


# word2Vec实现步骤
## 处理数据集
* **建立词语索引**
* **二次采样**（在一个背景窗口中，一个词和较低频率词同时出现比较高频率词同时出现对训练模型更有益）

## 提取中心词和背景词
* 将与中⼼词距离不超过背景窗口⼤小的词作为它的背景词
* 每次随机选择一个背景窗口大小

## 负采样
* 我们使⽤负采样来进⾏近似训练。对于⼀对中⼼词和背景词，我们随机采样K个噪声词（实验中
设K = 5）。
* 噪声词采样概率P(w)设为w词频与总词频之⽐的0.75次
⽅

## 训练padding