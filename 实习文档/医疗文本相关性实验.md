# 医疗文本相关性实验


# 一.问题背景
## 1.背景
Google官方Release出来的BERT模型是基于通用语料库中Pre-train，由于垂直医疗搜索的需求，我们需要提高模型在医疗特定领域的预测能力。我们使用Google-Base的BERT初始化医疗-BERT的参数，并用医疗数据在其基础上继续进行Pre-train，最后在医疗垂直文本query-title Ranking任务上fine-tune测试效果。
## 2.参考论文

- Devlin, Jacob et al. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” _NAACL-HLT_ (2019). [[PDF]]([https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf))
- Burges, Christopher J. C.. “From RankNet to LambdaRank to LambdaMART: An Overview.” (2010).[[PDF]](https://pdfs.semanticscholar.org/0df9/c70875783a73ce1e933079f328e8cf5e9ea2.pdf)
- Miyato, T., Dai, A.M., & Goodfellow, I.J. (2016). Adversarial Training Methods for Semi-Supervised Text Classification. _ICLR_.[[PDF]] ([https://pdfs.semanticscholar.org/62b2/e0b1b8e90c4e299bf2096af242d048207217.pdf](https://pdfs.semanticscholar.org/62b2/e0b1b8e90c4e299bf2096af242d048207217.pdf))
  - 下图展示了对抗训练( Adversarial Training) ,训练时通过在模型的Embedding层加入一个扰动，并通过训练。使模型在加入扰动后预测的结果保持不变。通过这种方式，可以在一定程度上缓解过拟合，增强模型的鲁棒性。

![{6EA0C477-D949-4A92-A170-4D7877A3B42A}_20191114110338.jpg]()


# 二、Fine-tune任务描述
## 1.任务描述
Fine-tune任务为 **医疗垂直文本query-title Ranking任务，**采用LearningToRank中的Pairwise方法进行训练。验证数据为**医疗垂直满意度样本，**评价指标为**正逆序**。
## 2.模型描述
分别使用了不同层数的**BERT**和通过BERT蒸馏的**MIX**进行试验对比。


 
# 三、数据
## 1.数据描述

- 训练数据(Training Set)：由**11W**左右医疗垂直文本query-title对，生成**150W**左右Pair。
- 验证数据(Validation Set): 由2700 query，2Wquery-title组成。
- 训练数据地址: hdfs://sm-hadoop/user/kangping.yinkp/zengqinghong/medical_Rev/med_ranking/med_Ranking/train
## 2.说明
由于训练标注数据较少，实验中加入了一版对抗训练进行对比，测试其效果。
# 
# 四、代码简介
## 1.代码地址

- BERT实验代码地址:[http://lego.sm.cn/ide?project_id=3391](http://lego.sm.cn/ide?project_id=3391)
- MIX蒸馏实验代码地址:[http://lego.sm.cn/ide?project_id=3610](http://lego.sm.cn/ide?project_id=3610)

## 2.对抗样本训练说明
对抗样本训练的代码文件为bert_algorithm_adv_ranking.py 以及 model_adv.py。

- model_adv.py

参照论文，代码中在BERT word_embedding加入扰动。
```python
    with tf.variable_scope("bert", scope, partitioner=tf.min_max_variable_partitioner(max_partitions=num_ps_replicas)):
      with tf.variable_scope("embeddings"):
        # Perform embedding lookup on the word ids.
        (self.word_embedding, self.embedding_table) = embedding_lookup(
            input_ids=input_ids,
            vocab_size=config.vocab_size,
            embedding_size=config.hidden_size,
            initializer_range=config.initializer_range,
            word_embedding_name="word_embeddings",
            use_one_hot_embeddings=use_one_hot_embeddings)
        # 
        """##########################################################"""
        """if use adversarial training,compute noise"""
        if adv_noise is not None:
            adv_noise_norm = tf.nn.l2_normalize(adv_noise,dim=[1,2])
            adv_noise_final = adv_noise_norm * adv_epsilon
            self.word_embedding = self.word_embedding + adv_noise_final
		"""##########################################################"""
		
        # Add positional embeddings and token type embeddings, then layer
        # normalize and perform dropout.
        self.embedding_output = embedding_postprocessor(
            input_tensor=self.word_embedding,
            use_token_type=True,
            token_type_ids=token_type_ids,
            token_type_vocab_size=config.type_vocab_size,
            token_type_embedding_name="token_type_embeddings",
            use_position_embeddings=True,
            position_embedding_name="position_embeddings",
            initializer_range=config.initializer_range,
            max_position_embeddings=config.max_position_embeddings,
            dropout_prob=config.hidden_dropout_prob)

```

- bert_algorithm_adv_ranking.py

adv_noise为上面加入的noise，计算方法如下面代码所示 ，论文中公式为
![{02854420-0652-4736-8454-69CB0B6EC952}_20191114113608.jpg]()
```python
            """Adversarial training pipline"""
            #step 1. compute the gradient

            grads = tf.gradients(tf.reduce_mean(logloss),model1.word_embedding)
            grads = tf.stop_gradient(grads[0])
            
            #step 2. compute the adversarial sample logits, add r_adv
            with tf.variable_scope("reload") as scope:
                scope.reuse_variables()
                model1_adv = BertModel(
                    config = bert_config,
                    is_training = is_training,
                    input_ids = input_ids_1,
                    num_ps_replicas = context.cluster_info.num_ps_replicas,
                    input_mask = None,
                    token_type_ids = segmend_ids_1,
                    use_one_hot_embeddings = False,
                    adv_noise = grads)
                """reuse the model"""
                scope.reuse_variables()
                model2_adv = BertModel(
                    config = bert_config,
                    is_training = is_training,
                    input_ids = input_ids_2,
                    num_ps_replicas = context.cluster_info.num_ps_replicas,
                    input_mask = None,
                    token_type_ids = segmend_ids_2,
                    use_one_hot_embeddings = False,
                    adv_noise = grads)
```


# 五、实验记录
## 1.实验结果
**说明：**sm开头模型为神马提供的pre-train模型，damo开头的模型为达摩院提供的模型。adv为加入对抗训练，adamwd为使用BERT ADAM_WEIGHTDECAY优化器。使用的训练数据为12W的医疗标注数据


- **MIX** ：由BERT-12L-no-med蒸馏训练得到，其中Bert训练用的是600W的大搜标注数据
- **BERT-8L-no-med** : 未经过医疗Pre-train的BERT 8层模型
- **sm-12L-med** ：豫哥经过医疗数据Pre-train的BERT 12层模型
- **damo-6L-med** : 达摩院提供的经过医疗数据Pre-train的BERT 6层模型
- **damo-4L-med** ：达摩院提供的经过医疗数据Pre-train的BERT 4层模型
- **damo-4L-no-med** : 达摩院提供的未经过医疗数据Pre-train的BERT 4层模型

| 模型名称 | 层数 | 训练Epoch | 初始学习率 | 是否使用对抗训练 | 是否使用医疗Pre-train | 优化器 | 正逆序 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| MIX |  | 5 | 1e-5 | 否 | 否 | ADAM | 1.77 |
| BERT-8L-no-med | 8 | 2 | 5e-5 | 否 | 否 | ADAM | 2.05 |
| sm-12L-med | 12 | 8 | 5e-5 | 否 | 是 | ADAM | 1.777 |
| sm-12L-med | 12 | 2 | 5e-5 | 否 | 是 | ADAM | 1.667 |
| **damo-6L-med** | 6 | 2 | 5e-5 | 否 | 是 | ADAM | **2.075** |
| damo-4L-med | 4 | 20 | 5e-6 | 否 | 是 | ADAM | 1.733 |
| damo-4L-med | 4 | 4 | 1e-5 | 否 | 是 | ADAM | 1.830 |
| **damo-4L-med** | 4 | 8 | 1e-5 | 否 | 是 | ADAM_WD | **1.882** |
| damo-4L-med-adv | 4 | 20 | 5e-6 | 是 | 是 | ADAM | 1.595 |
| damo-4L-med-adv | 4 | 4 | 1e-5 | 是 | 是 | ADAM | 1.703 |
| **damo-4L-med-adv** | 4 | 8 | 1e-5 | 是 | 是 | ADMA_WD | **1.87** |
| **damo-4L-no-med** | 4 | 20 | 5e-6 | 否 | 否 | ADAM | 1.408 |


## 2.实验分析

- 目前最佳模型 :  **damo-6L-med **
- 医疗Pre-train分析 ：
  - 纵向比较 : **damo-6L-med **效果和 BERT-8L-no-med 效果相当，可以看出医疗数据Pre-train对医疗专项的finetune任务有一定提高
  - 横向比较 : 同样的超参数配置下 **damo-4L-med **和 **damo-4L-no-med **在验证数据正逆序分别为**1.733**以及**1.408，**可以看出相对于未Pre-train的模型，damo-4L-med 有 20％左右的提升。
- 学习率分析 ：

该任务对学习率比较敏感，不同的学习率训练模型最后的效果也不同

- 对抗训练分析:

实验结果表示对抗训练没有明显提升，猜测原因

  1. 原始数据为AC标注数据，转换为TS目标时存在一定的标注错误，可能会造成影响
  1. 数据量太少
  1. 超参数设置问题，当学习率为1e-5并使用ADAM_WD，训练8-epoch时，使用对抗训练和不使用对抗训练结果差不多。学习率为1e-5并使用ADAM，训练4-epoch时，使用对抗训练比不使用对抗训练差。
  1. 代码写的有问题....(XD)
- 优化器分析:

可以看出使用BERT自带的优化器Adam_WeightDecay 比使用TensorFlow原生Adam优化器效果要好。

# 六、接下来还可以做的方向

1. 数据清洗，AC标注数据转换为TS标注数据存在一定的标注偏差
1. 知识蒸馏，用医疗数据Pre-Train的BERT大模型蒸馏层数参数较少的BERT小模型，保证推断速度
1. 引入知识图谱的信息，做任务增强




