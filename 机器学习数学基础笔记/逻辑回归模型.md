[TOC]
# 简单介绍逻辑回归模型
* **逻辑回归假设数据服从伯努利分布，通过极大化似然函数，运用梯度下降的方法求解参数**

# 逻辑回归与SVM比较
* 相同点
    * 都是分类算法
    * 都是有监督学习
    * 都是判别模型
* 不同点
    * 逻辑回归对异常值敏感，SVM对异常值不敏感
    * 损失函数不同，逻辑回归是交叉熵损失，SVM是合页损失
    * 逻辑回归在训练时所有点都有效，SVM只有支持向量有效
    * 训练集比较小的时候，使用SVM更好

# 逻辑回归如何处理非线性分类问题
* 使用kernel trick
* 将模型表示为`$\sum_i a_i<x_i,x> + b $`形式，对偶形式需要我们存储各个`a_i`,还需要保存`x_i`本身，存储量大
* 上面形式与SVM类似，但是SVM的对偶形式解时稀疏的，只有支持向量的`a_i`才非零
* 特征离散化然后交叉组合

# 逻辑回归优缺点
## 优点
* 模型简单，易于实现
* 可解释性好
* 超参数少

## 缺点
* 对于非线性数据效果不好
* 类别数很多的话，处理比较复杂
* 具有不相关或者高度相关的特征时效果较差

# 为什么逻辑回归比决策树更加容易处理高维稀疏特征
* 决策树正则项一般是树的深度和叶子节点个数，对高维稀疏特征很容易划分，惩罚小，容易过拟合
* 逻辑回归是特征的线性组合，正则项是对权重的惩罚，不会让某一个权重过大，而且很多特征为0，那么当前那维的权重，也对结果没有影响，模型不容易过拟合

# 逻辑回归适用场景
* 特征为数值型特征（不是直接的文本特征）
* 需要可解释性好的模型，逻辑回归输出值是一个概率，而且参数表示每个特征的权重
* 类别数少，而且数据线性可分
* 一般可以作为baseline