[TOC]
# AdaBoost流程
* AdaBoost学习过程是一种前向分布式算法，每轮迭代会生成一个弱分类器，最后由这些弱分类器加权组合得到最终的分类器
* 初始化每个训练样本的权重；如果有N个样本，每个训练样本的最初权重都是`1/N`
* 训练弱分类器，并且如果某一个样本在当前分类器被正确分类，那么下次训练时它的权重会被降低，如果当前样本被错误分类，那么它的权重会被提高，并且根据误差率确定当前分类器的权重
* 之后重复上述过程迭代。

# 集成学习适用场景
* 当训练数据量太大，以至难以使用单一分类器训练时，可以将数据按照一定策略划分为较小的子集。然后使用每个子集来训练单独的分类器，再用恰当的方法对分类器输出进行组合
* 如果训练数据量太小，那么使用bootstrapping技术可以从总体数据集中有放回的随机采样获得多个样本集，每个样本集作为训练集对分类器进行训练，这些样本集可以看作是从总体分布中得到的独立样本集。