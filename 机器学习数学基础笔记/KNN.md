[TOC]
# 简单介绍一下KNN
* KNN是一种有监督学习的算法，可以用来解决分类或者回归问题，它的基本法则是：**相同类别的样本在特征空间应当聚集在一起**。当给定一个新样本时，首先在训练集中需要距离新样本最邻近的k个样本，并把该样本标签预测为这最近样本中最多类别数的类别。

# 影响KNN算法的主要因素
* k值选择
* 距离度量
* 分类决策

# 如何选取KNN中的K值
* **选取较小的k值**，预测方差会增大，容易受噪声影响
* **选取较大的k值**，预测偏差会增大
* 一般使用交叉验证来选取

# KNN中的距离度量
* 欧式距离 p=2
* 曼哈顿距离 p=1
* 切比雪夫距离 p=无穷,是各个坐标距离的最大值

# 什么是KD树？怎么构建
* kd树是对数据点在k维空间中划分的一种数据结构，主要用于多维空间关键数据的搜索。本质上，**kd树就是一种平衡二叉树。（多维二叉查找树）**
* **构建方法**
    * 选定数据X1的Y1维数值a1做为根节点比对值，对所有的数值在Y1维进行一层BST排列。相当于根据Y1维数值a1对数据集进行分割。
    * 选定数据X2的Y2维数值a2做为根节点比对值，对所有的数值在Y2维进行一层BST排列。也即将数据集在Y2维上又做了一层BST
![](https://img-blog.csdn.net/20180203173238899?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMjQyMzg2NQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

* **树构建中的问题**
    * 如何决定每次根据哪个维度对子空间进行划分呢
        * 通常来说，更为分散的维度，我们就更容易的将其分开，是以这里我们通过求方差，用方差最大的维度来进行划分——这也就是最大方差法
    * 如何选定选定根节点对比数值呢？
        * 所以这里用当前维度的中值是比较合理的。 

# KD树和BST的区别
* BST的每个节点存储的是值，而Kd-Tree的根节点和中间节点存储的是对某个维度的划分信息，只有叶节点里才是存储的值。

# KD树的搜索
* 递归的找到叶子节点,找到候选最近点
* 沿着路径回溯，画圆，判断父节点是否与平面交割，来判断是否要进入到另一个子树查找
* 重复上述步骤，找到最近点


# KNN优缺点
## 优点
* 简单易于实现
* 没有显示的训练过程
* 多分类问题也容易解决

## 缺点
* 数据集较大时，预测速度慢
* 难以处理高维数据
* 需要对特征进行归一化
* 在不平衡的数据集上效果不好
* 对异常值、缺失值敏感。

# KNN使用场景
* 训练数据是增量式，时不时有新的训练数据进来
* 类别标签会随时间变换
* 需要一个容易解释的模型