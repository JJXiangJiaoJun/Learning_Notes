[TOC]
# 决策树的理解
* 通过学习，使模型在训练样本上的不确定性变为最低

# 决策树三要素
* 特征选择
* 决策树分裂准则
* 决策树的修剪

# 剪枝处理
* 预剪枝
* 后剪枝

# 决策树为什么是常用的基分类器
* 决策树可以方便的将样本权重整合到训练过程中
* 决策树的表达能力和泛化能力，可以通过调节树的深度来折中
* 数据样本的扰动对于决策树的影响较大，因此对于不同子样本集合生成的决策树基分类器随机性较大

# 可否将随机森林中的基分类器，由决策树替换为线性分类器或K-近邻
* 随机森林属于bagging集成学习，主要目的是降低分类器方差。bagging所采用的基分类器最好是对数据样本敏感的，线性分类器或者K-近邻分类器都是较为稳定的分类器，本身方差就不大，所以不适合作为基分类器。

# XGBoost如何处理缺失值
* 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。
* 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
* 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。


# 决策树的优缺点
## 优点
* 不需要归一化或者缩放数据
* 可以处理缺失值
* 模型可解释性好
* 可以进行特征选择

## 缺点
* 对于高维稀疏数据容易过拟合
* 对数据敏感，容易受样本扰动
* 样本较大时训练时间较长

# 决策树的使用场景
* 数据中具有噪声，缺失值等等
* 数据不是高维稀疏的数据
* 需要可解释性好的模型