[TOC]
# L1、L2正则化的解释
## 作用
* L1、L2正则化都是给参数加上限制，使得解变得更简单，是结构风险最小化的一种实现，主要用来防止过拟合
* 由于加入正则化后，模型参数会倾向选择比较简单的解，从而降低模型的复杂度，降低泛化误差，符合奥卡姆剃刀的原则

## 解释
* 从频率学派优化的角度来说，L1正则化是在损失函数优化时上加上了参数的L1范数，L2正则化是在损失函数优化时加上了L2范数

* 从贝叶斯学派角度来说，L1正则化是假定参数的先验为拉普拉斯分布，L2正则化是假定参数的先验为高斯分布

## 效果
* L1正则化得到的解会倾向于稀疏，L2正则化得到的解倾向于平滑


# 经验风险与结构风险
## 经验风险
* 模型损失函数在训练样本上的平均值，经验风险最小化就是对训练集中的样本损失函数最小化，经验风险越小表示模型在训练集拟合程度越小

## 结构风险
* 结构风险是在经验风险函数后面加上了一个对于模型复杂度的正则化惩罚项。因为经验风险越小，模型越复杂，包含的参数越多，也越容易过拟合，加入正则化之后希望能在经验风险和模型复杂度上找到一个折中，增强模型的泛化能力


# 为什么L1正则化会导致稀疏解
## 解空间形状
* L1正则化约束的解空间是多边形，优化的时候，多边形的解空间更加容易在坐标轴尖角处碰撞出稀疏解

## 从贝叶斯先验角度分析
* L1正则化相当于假设参数服从拉普拉斯先验，拉普拉斯先验在`0`处是一个尖峰，所以拉普拉斯先验分布中参数取值为0的可能性要更高

## 从数据计算的角度
* 引入L1正则化，损失函数的导数在0的位置有一个突变，所以0处是一个极小值点，更容易优化到这点

# L1正则化不可导的点如何处理
* 近端梯度下降
* 坐标下降法
* 交替乘子法