本文档整理记录算法面试过程中，对深度学习方面的常问问题。

1. **Faster-RCNN YOLO SSD原理**
2. **如何减少过拟合**
    + 正则化,weight decay
    + dropout
    + early stopping
    + Data Augmentation
    + Batch norm
    + 减小网络参数
    + ensemble方法
    + 贝叶斯方法（加入先验知识）
3. **CNN反向传播（卷积、池化处理，链式求导法则）**
4. **各种优化算法（Adam，Adgrad）**
    + AdaGrad
    + RMSProp
    + AdaDelta
    + Adam
5. **梯度消失梯度爆炸解决方案**
    + 预训练+微调
    + 残差连接
    + 使用Relu等激活函数
    + LSTM
    + Batchnorm等
6. **初始化**
    + 高斯分布
    + Xaiver
    + He参数初始化
    + 均匀分布
    + fine-tuning，预训练模型参数来进行初始化
7. **batch normalization原理**
8. **各种NMS算法的原理**
9. **LSTM为什么能解决梯度消失问题**
    + RNN发生梯度消失与梯度爆炸的原因是，梯度反传过程中，存在`$\partial H_t/\partial H_{t-1}$`多项连乘，当激活激活函数是`tanh时`，这项等于`tanh`导数乘以`$W_{hh}$`参数矩阵，`tanh`导数小于1,当`$W_{hh}$`小于1时，容易发生梯度消失，反之容易发生梯度爆炸
    + 我们注意到， 首先三个门的激活函数是sigmoid， 这也就意味着这三个门的输出要么接近于0 ， 要么接近于1。
    + 这样梯度反传时当门为1时，可以很好解决梯度消失问题，反之可以很好解决梯度爆炸问题
10. **深度学习调参经验**
    * 首先定位问题
        * **完全不收敛**，**效果不好** 
            * 完全不收敛：错误的输入数据，网络错误
            * 效果不好：欠拟合和过拟合
        * 可以绘制比较模型训练过程中，在训练集和验证集上的精度曲线，判断模型是过拟合函数欠拟合
        * 观察loss，如果loss突然出现波动，可能是由于输入样本数据存在错误
        * 可以先参考论文，按照论文给出的参数初始化
        * 如果找不到参考那么可以从学习率这种影响比较大的参数，然后到dropout，正则化项等等
    * **数据方面**
        * 选取一小部分数据做验证实验，加快速度 
        * 数据增广
        * 数据清洗
        * 样本随机打乱
        * 从badCase分析问题
    * 参数方面
        * 从影响比较大的参数开始调节
        * 先确定对数尺度。
    * 模型方面
        * 可以先**小规模数据，模型参数比较大**防止欠拟合
        * 检查Loss
        * 检查激活函数
11. **word2Vec和fastText区别和联系**
* fastText在word2Vec上引入了构词法，将一个单词当成由字符构成的n元语法
* 每个词是由各个字词的词向量相加得到
12. **word2Vec和Glove区别**
* word2Vec是基于局部语料库训练的，其特征提取是基于滑窗的；而glove是基于全局语料，有全局信息
* word2Vec用的是交叉熵损失。glove用的是平方损失函数
13. **训练时网络不收敛原因**
* 数据和标签
* 学习率设置不合理
* 网络设定不合理
* 数据label有错
* 数据需要归一化
* 可能存在梯度消失
14. **NLP中的数据增强方法**
* 同义词替换
* 翻译
* 随机丢弃和shuffle
* GAN生成文本
* 使用预训练的语言模型生成文本
* pesudo-labeling
15. **深度学习为什么不用二阶优化**
* 计算量大，训练慢
* 二阶方法能够更快地求得更高精度的解，这在浅层模型是有益的。而在神经网络这类深层模型中对参数的精度要求不高，甚至不高的精度对模型还有益处，能够提高模型的泛化能力。
* 稳定性：二阶方法能更快求高精度的解，同样对数据本身要的精度也会相应的变高，这就会导致稳定性上的问题。
16. **为什么CNN可以用来做自然语言处理**
* CNN擅长捕获局部相关性，具体到自然语言处理中，可以用来提取句子中类似与n-gram的特征。 
17. **NLP中的文本预处理**
* 分词，大小写转换，标点符号转换，去除停用词
* 特征构建（词袋模型、tfidf）
* 特征降维
18. 