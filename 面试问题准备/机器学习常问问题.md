[TOC]
1. **数据不均衡处理方法**
    + 重采样
        + 上采样
        + 下采样 + ensemble
    + 数据合成
    + 加权
    + 分类阈值移动

2. **梯度消失和梯度爆炸原因**
    + 出现原因
        + 神经网络使用反向传播算法来更新权重，而BP算法中的链式求导法则会导致梯度消失或梯度爆炸
        + 网络层数太深
        + 选取了不恰当的激活函数
    + 解决办法
        + 预训练+微调
        + 梯度剪切、正则
        + 选用relu等激活函数
        + batchnorm
        + 残差结构
        + LSTM结构
3. **过拟合出现原因及解决办法（模型泛化能力差）**
    + 表现现象
        + 模型在训练集上表现很好，但是在交叉验证集上表现先好后差
    + 出现原因
        + 训练集的数量级和模型的复杂度不匹配。训练集数量级要小于模型复杂度
        + 训练集和测试集的分布不一致
        + 样本噪音数据干扰过大
        + 学习迭代次数过多
    +   解决方法
        + 正则化
        + dropout
        + 数据增广
        + 调小模型复杂度
        + early stoppping
        + 集成学习。bagging或者boosting
        + batch norm
        + 贝叶斯方法（添加高斯先验）
        + label smooth（标签平滑）

3. **特征选择方向**
+ Pearson 相关系数
+ 互信息和最大信息系数
+ 距离相关系数
+ 基于学习模型的特征排序（filter、wrapper）
 
4. **PR曲线和ROC曲线以及AUC和mAP**
    + PR曲线
    <br/>PR曲线下的面积为AP（average-precision,AP）mAP:计算每个类别的AP，最后取平均
        + 给定一些样本，模型预测的有多准 
    + ROC曲线（受试者工作特征曲线）
    <br/>横轴为FPR，纵轴为TPR（recall）ROC曲线与坐标轴围住的面积被称为AUC
        + 随机给定一个正样本和一个负样本，正样本预测概率大于负样本预测概率的程度。

+ ROC曲线兼顾正例和负例，所以适合评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。
+ ROC对类别分布不敏感，PR曲线对类别分布敏感。
+ 类别不平衡时，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线好
+ 注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。
## 树模型方面
1. **ID3、C4.5、CART比较**
    + **决策树学习**：决策树学习的损失函数通常是正则化的极大似然函数
    + **特征选择**
        + **ID3 信息增益**：`$g(D,A)=H(D)-H(D|A)$`, 信息增益偏向**取值较多的特征**
        + **C4.5 信息增益率**：`$g_R(D,A) = \frac {g(D,A)}{H_A(D)}$` ,其中`$H_A(D)$`是将当前特征A作为随机变量（取值是特征A的各个特征值进行划分），信息增益率偏向**取值较少的特征**
        + **CART Gini指数 或者样本方差**：`$Gini(D) = 1 - \sum {(\frac {|C_k|}{D})^2}$` ,基尼系数越小，则不纯度越低，特征越好
    + **树的生成**
        + **ID3**：应用 信息增益 准则选择特征，从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；
            + 不能对连续数据进行处理，只能通过连续数据离散化进行处理
            + 采用信息增益进行数据分裂容易偏向取值较多的特征，准确性不如信息增益率；
            + 无法处理缺失值
            + 没有采用剪枝
        + **C4.5**：用信息增益率选择特征，在树的构造过程中会进行剪枝操作优化，能够自动完成对连续属性的离散化处理；
            + 解决连续值不能处理问题，比如，m个样本的连续特征A有m个，从小到大排列为a1,a2,a3......am,则C4.5取相邻两样本值的平均值，一共取得m-1个划分点
            + 可处理缺失值
            + 引入剪枝
            + 对数据进行多次顺序扫描和排序，效率较低；只适合小规模数据集，需要将数据放到内存中。
        + **CART**：CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。
            + 采用调整权重来处理缺失值 
2. **为什么gbdt不适合处理sparse特征**
    + 树模型惩罚项通常为叶子节点和深度等，对sparse特征，树模型可以轻松分开，惩罚较小，容易造成过拟合。
3. **决策树的损失函数**
* 决策树的损失函数为经验熵
```math
L = \sum_{t=1}^TN_tH_t(T) + \alpha|T| 
```
*其中`$T$`为叶节点个数，`$N_t$`为第t个叶节点的样本个数，`$H_t(T)$`为第t个叶节点的经验熵.
4. **随机森林、GBDT、XGBoost和LightGBM**
* 随机森林
    * 随机森林的集成学习方法是**bagging**，但是和bagging不同的是bagging只使用bootstrap有放回的采样样本，但是随机森林不仅随机选择样本，也随机选择藤真，因此防止过拟合能力更强，降低方差
    * **并行算法**，因为不同基学习器之间独立
    * bagging主要关注降低**方差**
* GBDT
    * **GBDT中所有树都是回归树，不是分类树**
* XGBoost与GBDT不同
    * **基函数**，GBDT只用CART树，XGBoost除CART外，也支持线性函数
    * **目标不同**，GBDT和XGBoost都是根据目标增益分裂节点，GBDT最小化均方误差，XGboost进一步引入正则项
    * **正则化**，XGBoost正则项包含对叶子节点约束以及叶子节点输出权重的L2范数
    * **导数阶数不同**，GBDT利用一阶泰勒展开，XGBoost利用二阶泰勒展开可以加速收敛
    * **最佳特征选取策略不同**，GBDT遍历所有特征，XGBoost引入类似于随机森林子采样，有利于防止过拟合加速运算
    * **候选分裂点选取策略不同**，GBDT遍历所有特征取值，XGBoost会根据二阶导数选择样本分位点作为候补
    * **Boosted模型构成不同**，XGBoost为每棵树设定了系数，表示不完全信任
    * **XGBoost**,支持并行，注意xgboost不同于随机森林中的并行粒度是：tree，xgboost与其他提升方法（比如GBDT）一样，也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。

5. **简单介绍一下XGBoost**
* 首先需要说一说GBDT，它是一种基于Boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪学习，每次迭代生成一颗新的树，来拟合之前t-1棵树的负梯度。
* XGBoost是对GBDT进行了一系列的优化，比如利用了损失函数二阶泰勒展开，目标函数加入了正则项，特征预排序等等。

6. **XGBoost调参一般步骤**
* **初始化基本变量**
    * 最大深度
    * 正则化
    * 列采样的概率
* **确定learning rate以及estimator数量**
* **max_depth和min_child_weight**
7. 
----

6. **为什么要用多层神经网络而不用单层**
    + 要达到同样的效果，单层神经网络的参数相比于多层神经网络非常大
7. **mobilenet分离卷积**
8. **K-means和EM关系**
    + k-means是高斯混合聚类在混合成分方差相等，且每个样本仅指派一个混合成分时的特例
9. **PCA、SVD等原理**
10. **模型压缩方法**
11. **朴素贝叶斯相关问题**
    * **拉普拉斯平滑**，对于某个数据集，我们考虑到对于某个特征X在训练集中没有出现，则会导致整个分类概率变为0，采用+1的办法来进行平滑
    * **多项式模型**，在多项式模型中，设某文档d={t1,t2,...,tk}，ti(i=1,2,...,k)为在该文档d中出现的单词，允许重复。
        *   **先验概率p(c)** = 类c下单词总数 / 整个训练样本的单词总数
        *   **类条件概率 p(tk|c)** = (类c下单词tk在各个文档出现的数量之和+1) / (类c下单词总数 + |V|)
        *  V是训练样本中所有单词的集合(set，即每个单词有且仅能出现一次)，即该训练样本的词汇表。
    * **伯努利模型**
        * 在伯努利模型中，每个特征的取值是布尔型的，即true和false，或者1和0。在文本分类中，就是一个特征有没有在一个文档中出现。
        * **先验概率p(c)** =类c下文档总数/整个训练样本的文档总数
        * **类条件概率p(tk|c)** =类c下包含单词tk的文档总数/类c下的文档总数

V是训练样本中所有单词的集合(set，即每个单词有且仅能出现一次)，即该训练样本的词汇表。

12. **如何跳出局部最优**
    + 用多组不同的参数初始化训练
    + 模拟退火
    + 带动量的梯度下降
13. **各种优化器比较**
    + 梯度下降
        + 优点：可以得到理论全局最优解
        + 缺点：运行速度慢，内存不够
    + SGD
        + 优点：可以在一定程度上解决局部最优解
        + 缺点：容易震荡
    + mini-batch SGD
        + 优点：收敛速度较快
        + 缺点：容易陷入局部最优
    + 牛顿法
        + 优点：收敛快
        + 缺点：需要计算Hessian矩阵
14. **区分bootstrap、bagging、boosting和adaboost**
15. **常见的特征降维算法**
    + 主成成分分析（PCA）
    + 线性判别分析（Linear Discriminant Analysis,LDA）
    + 局部线性嵌入(Locally Linear Embedding,LLE)
    + 多维缩放（Multidimensional Scaling,MDS）
    + 等度量映射（Isometric Mapping，Isomap)
    + 局部保留投影(Locality Preserving Projections,LPP)
    + 拉普拉斯特征映射(Laplacian Eigenmaps,LE)
16. **分布式训练之同步更新和异步更新**
    + **同步更新：** 在同步更新的时候， 每次梯度更新，要等所有分发出去的数据计算完成后，返回回来结果之后，把梯度累加算了均值之后，再更新参数。这样的好处是loss的下降比较稳定， 但是这个的坏处也很明显， 处理的速度取决于最慢的那个分片计算的时间。
    + **异步更新：** 在异步更新的时候， 所有的计算节点，各自算自己的， 更新参数也是自己更新自己计算的结果， 这样的优点就是计算速度快，计算资源能得到充分利用，但是缺点是loss的下降不稳定，抖动大。
17. **主流搜索引擎的模糊匹配**
    + 搜索query分词
    + 相似query挖掘
    + 纠错
    + 查询改写
        + 相似query
        + 动态丢词
        + 意图预测
18. **特征离散化的优势**
    + 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型
    + **离散特征的增加或者减少都很容易，由于模型的快速迭代**，(离散特征的增加和减少，模型也不需要调整，重新训练是必须的，相比贝叶斯推断方法或者树模型方法迭代快)
    + 稀疏向量内积乘法运算速度快，计算结构方便存储
    + 离散化后的特征对异常数据有很强的鲁棒性，比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；离散化后年龄300岁也只对应于一个权重，如果训练数据中没有出现特征"年龄-300岁"，那么在LR模型中，其权重对应于0
    + 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；在LR模型中，特征A作为连续特征对应的权重是Wa。A是线性特征，因为`y = Wa*A,y对于A的导数就是Wa,如果离散化后，A按区间离散化为A_1,A_2,A_3。那么y = w_1*A_1+w_2*A_2+w_3*A_3`.那么y对于A的函数就相当于分段的线性函数，y对于A的导数也随A的取值变动，所以，相当于引入了非线性。
    + 离散化后可以进行特征交叉，加入特征A 离散化为M个值，特征B离散为N个值，那么交叉之后会有M*N个变量，进一步引入非线性，提升表达能力
    + .特征离散化后，模型会更稳定，比如：用用户年龄离散化后，20-30作为一个区间，不会因为一个用户年龄增长1岁变成完全不同的人，当然处于区间相邻的样本会刚好相反，所以划分是门学问
    + 特征离散化以后，起到了简化逻辑回归模型的的作用，降低模型过拟合的风险。
19. **什么情况下要特征归一化**
+ 为什么要归一化
    + **特征间的单位（尺度）可能不同**
    + **因尺度差异，其损失函数的等高线图可能是椭圆形**
+ 什么情况下要进行归一化
    + 涉及**距离计算的方法**，比如K-means、KNN、PCA、SVM等，一般需要feature scaling
    + **损失函数中含有正则项时**，一般需要feature scaling
    + **梯度下降算法，需要feature scaling**
+ 什么时候不需要归一化
    + 与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；
    + 与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。
20. **BadCase处理**
* **针对bad case增加训练样本。**
* **另外，减少噪音数据，这个其实挺关键的，通过规则减少噪音比增加数据本身效果提升可能更高。**
* **另外有一些case其实可以通过规则去解决，这种case就用规则去解决吧**
* **数据层面**，提升数据质量，宁可错杀不要放过的做，如果能通过外部语料进一步扩充数据规模（这估计是NLP的基础操作了），尽可能扩充，不建议使用模板构建，但是实在不行增加一点其实无妨，但不能太多
* **特征层面**，除了文本层面的特征，词向量之类的，还有词性、term weighitng、位置、TF-IDF等特征可以加上试试。
21. **为什么分类使用交叉熵而不用平方损失**
* （均方误差）对于每一个输出的结果都非常看重，而交叉熵只对正确分类的结果看重。
22. **特征工程分为哪几步**
* 数据预处理
    * 异常值
    * 缺失值
    * 归一化
    * 数据重复
    * 数据不平衡问题
* 特征选择
    * 特征是否发散
    * 特征是否与问题相关
* 特征提取
    * 特征降维   
23. **从10亿的url中，找出出现最多次的10条**
* 用哈希即可
24. **dropout和bagging区别**
* 在 Bagging 的情况下，所有模型都是独立的；而在 Dropout 的情况下，所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。
* 在 Bagging 的情况下，每一个模型都会在其相应训练集上训练到收敛。而在 Dropout 的情况下，通常大部分模型都没有显式地被训练；取而代之的是，在单个步骤中我们训练一小部分的子网络，参数共享会使得剩余的子网络也能有好的参数设定。
25. 
